{"epoch1": {"rightWeight": "tensor([[ 0.0666, -0.0034, -0.0786],\n        [ 0.0668,  0.0184, -0.1469],\n        [ 0.1045,  0.0801,  0.0295],\n        [-0.0734,  0.0823,  0.0284]], grad_fn=<SumBackward1>)", "leftWeight": "Parameter containing:\ntensor([[ 0.4172,  0.5617,  0.5075],\n        [-0.4541,  0.4139, -0.0069],\n        [-0.5111, -0.4123, -0.5061],\n        [-0.2071, -0.3320, -0.2682],\n        [ 0.1904,  0.5394, -0.1983]], requires_grad=True)", "leftBias": "Parameter containing:\ntensor([ 0.0768,  0.1506,  0.4036,  0.3674, -0.3453], requires_grad=True)", "modelOut": "tensor([[ 0.0628,  0.1194,  0.4107,  0.3758, -0.3189],\n        [ 0.0405,  0.1288,  0.4362,  0.3869, -0.2936],\n        [ 0.1804,  0.1360,  0.3022,  0.3113, -0.2881],\n        [ 0.1068,  0.2178,  0.3928,  0.3477, -0.3205]],\n       grad_fn=<AddmmBackward0>)", "log_probs": "tensor([[-1.7092, -1.6526, -1.3613, -1.3962, -2.0909],\n        [-1.7418, -1.6535, -1.3461, -1.3954, -2.0759],\n        [-1.5794, -1.6237, -1.4576, -1.4485, -2.0479],\n        [-1.6814, -1.5704, -1.3954, -1.4405, -2.1087]],\n       grad_fn=<LogSoftmaxBackward0>)", "y_pred": "tensor([[-1.7092, -1.6526, -1.3613, -1.3962, -2.0909],\n        [-1.7418, -1.6535, -1.3461, -1.3954, -2.0759],\n        [-1.5794, -1.6237, -1.4576, -1.4485, -2.0479],\n        [-1.6814, -1.5704, -1.3954, -1.4405, -2.1087]],\n       grad_fn=<LogSoftmaxBackward0>)", "loss": "tensor(1.3991, grad_fn=<NllLossBackward0>)"}, "epoch2": {"rightWeight": "tensor([[ 0.0658,  0.0174, -0.1479],\n        [ 0.0656, -0.0044, -0.0796],\n        [-0.0744,  0.0813,  0.0274],\n        [ 0.1035,  0.0791,  0.0285]], grad_fn=<SumBackward1>)", "leftWeight": "Parameter containing:\ntensor([[ 0.4162,  0.5607,  0.5085],\n        [-0.4551,  0.4129, -0.0059],\n        [-0.5101, -0.4133, -0.5071],\n        [-0.2081, -0.3310, -0.2672],\n        [ 0.1894,  0.5384, -0.1973]], requires_grad=True)", "leftBias": "Parameter containing:\ntensor([ 0.0758,  0.1496,  0.4046,  0.3684, -0.3463], requires_grad=True)", "modelOut": "tensor([[ 0.0378,  0.1277,  0.4388,  0.3885, -0.2953],\n        [ 0.0602,  0.1183,  0.4133,  0.3775, -0.3205],\n        [ 0.1044,  0.2169,  0.3950,  0.3497, -0.3220],\n        [ 0.1777,  0.1349,  0.3046,  0.3131, -0.2898]],\n       grad_fn=<AddmmBackward0>)", "log_probs": "tensor([[-1.7447, -1.6548, -1.3437, -1.3940, -2.0778],\n        [-1.7120, -1.6539, -1.3589, -1.3947, -2.0927],\n        [-1.6840, -1.5715, -1.3934, -1.4387, -2.1104],\n        [-1.5821, -1.6249, -1.4551, -1.4467, -2.0495]],\n       grad_fn=<LogSoftmaxBackward0>)", "y_pred": "tensor([[-1.7447, -1.6548, -1.3437, -1.3940, -2.0778],\n        [-1.7120, -1.6539, -1.3589, -1.3947, -2.0927],\n        [-1.6840, -1.5715, -1.3934, -1.4387, -2.1104],\n        [-1.5821, -1.6249, -1.4551, -1.4467, -2.0495]],\n       grad_fn=<LogSoftmaxBackward0>)", "loss": "tensor(1.3970, grad_fn=<NllLossBackward0>)"}, "epoch3": {"rightWeight": "tensor([[ 0.0648,  0.0164, -0.1489],\n        [ 0.0646, -0.0054, -0.0806],\n        [-0.0754,  0.0803,  0.0264],\n        [ 0.1025,  0.0781,  0.0275]], grad_fn=<SumBackward1>)", "leftWeight": "Parameter containing:\ntensor([[ 0.4152,  0.5597,  0.5095],\n        [-0.4561,  0.4119, -0.0049],\n        [-0.5091, -0.4143, -0.5081],\n        [-0.2091, -0.3300, -0.2662],\n        [ 0.1884,  0.5374, -0.1963]], requires_grad=True)", "leftBias": "Parameter containing:\ntensor([ 0.0748,  0.1486,  0.4056,  0.3694, -0.3473], requires_grad=True)", "modelOut": "tensor([[ 0.0350,  0.1265,  0.4414,  0.3901, -0.2971],\n        [ 0.0576,  0.1172,  0.4158,  0.3791, -0.3222],\n        [ 0.1019,  0.2159,  0.3973,  0.3517, -0.3235],\n        [ 0.1751,  0.1338,  0.3071,  0.3149, -0.2914]],\n       grad_fn=<AddmmBackward0>)", "log_probs": "tensor([[-1.7476, -1.6562, -1.3412, -1.3925, -2.0797],\n        [-1.7148, -1.6551, -1.3565, -1.3932, -2.0946],\n        [-1.6867, -1.5727, -1.3913, -1.4369, -2.1121],\n        [-1.5847, -1.6260, -1.4527, -1.4449, -2.0512]],\n       grad_fn=<LogSoftmaxBackward0>)", "y_pred": "tensor([[-1.7476, -1.6562, -1.3412, -1.3925, -2.0797],\n        [-1.7148, -1.6551, -1.3565, -1.3932, -2.0946],\n        [-1.6867, -1.5727, -1.3913, -1.4369, -2.1121],\n        [-1.5847, -1.6260, -1.4527, -1.4449, -2.0512]],\n       grad_fn=<LogSoftmaxBackward0>)", "loss": "tensor(1.3949, grad_fn=<NllLossBackward0>)"}}